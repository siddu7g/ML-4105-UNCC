{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "o08BQCAa1QGz",
        "outputId": "cacc2da7-75b8-4349-e7b9-afa064db1c9d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using device: cuda\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 170M/170M [00:03<00:00, 47.7MB/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training the network on GPU\n",
            "Epoch 1/100, Loss: 1.6601\n",
            "Epoch 2/100, Loss: 1.4735\n",
            "Epoch 3/100, Loss: 1.3853\n",
            "Epoch 4/100, Loss: 1.3184\n",
            "Epoch 5/100, Loss: 1.2479\n",
            "Epoch 6/100, Loss: 1.1914\n",
            "Epoch 7/100, Loss: 1.1423\n",
            "Epoch 8/100, Loss: 1.0836\n",
            "Epoch 9/100, Loss: 1.0406\n",
            "Epoch 10/100, Loss: 0.9933\n",
            "Epoch 11/100, Loss: 0.9467\n",
            "Epoch 12/100, Loss: 0.9011\n",
            "Epoch 13/100, Loss: 0.8702\n",
            "Epoch 14/100, Loss: 0.8116\n",
            "Epoch 15/100, Loss: 0.7887\n",
            "Epoch 16/100, Loss: 0.7525\n",
            "Epoch 17/100, Loss: 0.7126\n",
            "Epoch 18/100, Loss: 0.6950\n",
            "Epoch 19/100, Loss: 0.6476\n",
            "Epoch 20/100, Loss: 0.6394\n",
            "Epoch 21/100, Loss: 0.5944\n",
            "Epoch 22/100, Loss: 0.5779\n",
            "Epoch 23/100, Loss: 0.5602\n",
            "Epoch 24/100, Loss: 0.5536\n",
            "Epoch 25/100, Loss: 0.5147\n",
            "Epoch 26/100, Loss: 0.5123\n",
            "Epoch 27/100, Loss: 0.4808\n",
            "Epoch 28/100, Loss: 0.4797\n",
            "Epoch 29/100, Loss: 0.4612\n",
            "Epoch 30/100, Loss: 0.4390\n",
            "Epoch 31/100, Loss: 0.4271\n",
            "Epoch 32/100, Loss: 0.4232\n",
            "Epoch 33/100, Loss: 0.4044\n",
            "Epoch 34/100, Loss: 0.4092\n",
            "Epoch 35/100, Loss: 0.3948\n",
            "Epoch 36/100, Loss: 0.3682\n",
            "Epoch 37/100, Loss: 0.3789\n",
            "Epoch 38/100, Loss: 0.3592\n",
            "Epoch 39/100, Loss: 0.3669\n",
            "Epoch 40/100, Loss: 0.3674\n",
            "Epoch 41/100, Loss: 0.3163\n",
            "Epoch 42/100, Loss: 0.3600\n",
            "Epoch 43/100, Loss: 0.3201\n",
            "Epoch 44/100, Loss: 0.3423\n",
            "Epoch 45/100, Loss: 0.3197\n",
            "Epoch 46/100, Loss: 0.3192\n",
            "Epoch 47/100, Loss: 0.2983\n",
            "Epoch 48/100, Loss: 0.3049\n",
            "Epoch 49/100, Loss: 0.2971\n",
            "Epoch 50/100, Loss: 0.3141\n",
            "Epoch 51/100, Loss: 0.2750\n",
            "Epoch 52/100, Loss: 0.2948\n",
            "Epoch 53/100, Loss: 0.2800\n",
            "Epoch 54/100, Loss: 0.3043\n",
            "Epoch 55/100, Loss: 0.2536\n",
            "Epoch 56/100, Loss: 0.3041\n",
            "Epoch 57/100, Loss: 0.2635\n",
            "Epoch 58/100, Loss: 0.2683\n",
            "Epoch 59/100, Loss: 0.2502\n",
            "Epoch 60/100, Loss: 0.2581\n",
            "Epoch 61/100, Loss: 0.2396\n",
            "Epoch 62/100, Loss: 0.2824\n",
            "Epoch 63/100, Loss: 0.2499\n",
            "Epoch 64/100, Loss: 0.2316\n",
            "Epoch 65/100, Loss: 0.2681\n",
            "Epoch 66/100, Loss: 0.2641\n",
            "Epoch 67/100, Loss: 0.2399\n",
            "Epoch 68/100, Loss: 0.2196\n",
            "Epoch 69/100, Loss: 0.2740\n",
            "Epoch 70/100, Loss: 0.2290\n",
            "Epoch 71/100, Loss: 0.2631\n",
            "Epoch 72/100, Loss: 0.2069\n",
            "Epoch 73/100, Loss: 0.2085\n",
            "Epoch 74/100, Loss: 0.2617\n",
            "Epoch 75/100, Loss: 0.2254\n",
            "Epoch 76/100, Loss: 0.2078\n",
            "Epoch 77/100, Loss: 0.2024\n",
            "Epoch 78/100, Loss: 0.2551\n",
            "Epoch 79/100, Loss: 0.2116\n",
            "Epoch 80/100, Loss: 0.2431\n",
            "Epoch 81/100, Loss: 0.2021\n",
            "Epoch 82/100, Loss: 0.2408\n",
            "Epoch 83/100, Loss: 0.1988\n",
            "Epoch 84/100, Loss: 0.2453\n",
            "Epoch 85/100, Loss: 0.1991\n",
            "Epoch 86/100, Loss: 0.2054\n",
            "Epoch 87/100, Loss: 0.2178\n",
            "Epoch 88/100, Loss: 0.1817\n",
            "Epoch 89/100, Loss: 0.2241\n",
            "Epoch 90/100, Loss: 0.2434\n",
            "Epoch 91/100, Loss: 0.1873\n",
            "Epoch 92/100, Loss: 0.1991\n",
            "Epoch 93/100, Loss: 0.2100\n",
            "Epoch 94/100, Loss: 0.1763\n",
            "Epoch 95/100, Loss: 0.2238\n",
            "Epoch 96/100, Loss: 0.1942\n",
            "Epoch 97/100, Loss: 0.2100\n",
            "Epoch 98/100, Loss: 0.2194\n",
            "Epoch 99/100, Loss: 0.1979\n",
            "Epoch 100/100, Loss: 0.2060\n",
            "Training completed in 1197.63 seconds.\n",
            "Evaluating the network\n",
            "Validation Accuracy: 0.5038\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "import time\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(f\"Using device: {device}\")\n",
        "\n",
        "transform = transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
        "])\n",
        "\n",
        "trainset = torchvision.datasets.CIFAR10(root='./data', train=True, download=True, transform=transform)\n",
        "trainloader = torch.utils.data.DataLoader(trainset, batch_size=64, shuffle=True, num_workers=2)\n",
        "\n",
        "testset = torchvision.datasets.CIFAR10(root='./data', train=False, download=True, transform=transform)\n",
        "testloader = torch.utils.data.DataLoader(testset, batch_size=64, shuffle=False, num_workers=2)\n",
        "\n",
        "classes = ['airplane', 'automobile', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck']\n",
        "\n",
        "class SimpleNN(nn.Module):\n",
        "    def __init__(self, input_dim, hidden_dim, output_dim):\n",
        "        super(SimpleNN, self).__init__()\n",
        "        self.hidden = nn.Linear(input_dim, hidden_dim)\n",
        "        self.output = nn.Linear(hidden_dim, output_dim)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x.view(x.size(0), -1)\n",
        "        x = torch.relu(self.hidden(x))\n",
        "        x = self.output(x)\n",
        "        return x\n",
        "\n",
        "input_dim = 32 * 32 * 3\n",
        "hidden_dim = 512 # Hidden Layer\n",
        "output_dim = 10\n",
        "model = SimpleNN(input_dim, hidden_dim, output_dim).to(device)\n",
        "\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
        "\n",
        "def train_model(model, trainloader, criterion, optimizer, epochs):\n",
        "    start_time = time.time()\n",
        "    for epoch in range(epochs):\n",
        "        running_loss = 0.0\n",
        "        model.train()\n",
        "        for i, (inputs, labels) in enumerate(trainloader, 0):\n",
        "            inputs, labels = inputs.to(device), labels.to(device)\n",
        "            optimizer.zero_grad()\n",
        "            outputs = model(inputs)\n",
        "            loss = criterion(outputs, labels)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            running_loss += loss.item()\n",
        "\n",
        "        print(f\"Epoch {epoch + 1}/{epochs}, Loss: {running_loss / len(trainloader):.4f}\")\n",
        "\n",
        "    end_time = time.time()\n",
        "    training_time = end_time - start_time\n",
        "    print(f\"Training completed in {training_time:.2f} seconds.\")\n",
        "    return training_time\n",
        "\n",
        "def evaluate_model(model, testloader):\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        for inputs, labels in testloader:\n",
        "            inputs, labels = inputs.to(device), labels.to(device)\n",
        "            outputs = model(inputs)\n",
        "            _, predicted = torch.max(outputs, 1)\n",
        "            total += labels.size(0)\n",
        "            correct += (predicted == labels).sum().item()\n",
        "\n",
        "    accuracy = correct / total\n",
        "    print(f\"Validation Accuracy: {accuracy:.4f}\")\n",
        "    return accuracy\n",
        "\n",
        "epochs = 100\n",
        "print(\"Training the network on GPU\")\n",
        "training_time = train_model(model, trainloader, criterion, optimizer, epochs)\n",
        "\n",
        "print(\"Evaluating the network\")\n",
        "validation_accuracy = evaluate_model(model, testloader)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "PART B"
      ],
      "metadata": {
        "id": "qjeIBL7u8A_J"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "import time\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(f\"Using device: {device}\")\n",
        "\n",
        "transform = transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
        "])\n",
        "\n",
        "trainset = torchvision.datasets.CIFAR10(root='./data', train=True, download=True, transform=transform)\n",
        "trainloader = torch.utils.data.DataLoader(trainset, batch_size=64, shuffle=True, num_workers=2)\n",
        "\n",
        "testset = torchvision.datasets.CIFAR10(root='./data', train=False, download=True, transform=transform)\n",
        "testloader = torch.utils.data.DataLoader(testset, batch_size=64, shuffle=False, num_workers=2)\n",
        "\n",
        "classes = ['airplane', 'automobile', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck']\n",
        "\n",
        "class ExtendedNN(nn.Module):\n",
        "    def __init__(self, input_dim, hidden_dim1, hidden_dim2, hidden_dim3, output_dim):\n",
        "        super(ExtendedNN, self).__init__()\n",
        "        self.hidden1 = nn.Linear(input_dim, hidden_dim1)\n",
        "        self.hidden2 = nn.Linear(hidden_dim1, hidden_dim2)\n",
        "        self.hidden3 = nn.Linear(hidden_dim2, hidden_dim3)\n",
        "        self.output = nn.Linear(hidden_dim3, output_dim)\n",
        "\n",
        "    def forward(self, x):\n",
        "      # 3 Hidden Layers\n",
        "        x = x.view(x.size(0), -1)\n",
        "        x = torch.relu(self.hidden1(x))\n",
        "        x = torch.relu(self.hidden2(x))\n",
        "        x = torch.relu(self.hidden3(x))\n",
        "        x = self.output(x)\n",
        "        return x\n",
        "\n",
        "input_dim = 32 * 32 * 3 #3072\n",
        "hidden_dim1 = 512\n",
        "hidden_dim2 = 256\n",
        "hidden_dim3 = 128\n",
        "output_dim = 10\n",
        "model = ExtendedNN(input_dim, hidden_dim1, hidden_dim2, hidden_dim3, output_dim).to(device)\n",
        "\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
        "\n",
        "def train_model(model, trainloader, criterion, optimizer, epochs):\n",
        "    start_time = time.time()\n",
        "    for epoch in range(epochs):\n",
        "        running_loss = 0.0\n",
        "        model.train()\n",
        "        for i, (inputs, labels) in enumerate(trainloader, 0):\n",
        "            inputs, labels = inputs.to(device), labels.to(device)\n",
        "            optimizer.zero_grad()\n",
        "            outputs = model(inputs)\n",
        "            loss = criterion(outputs, labels)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            running_loss += loss.item()\n",
        "\n",
        "        print(f\"Epoch {epoch + 1}/{epochs}, Loss: {running_loss / len(trainloader):.4f}\")\n",
        "\n",
        "    end_time = time.time()\n",
        "    training_time = end_time - start_time\n",
        "    print(f\"Training completed in {training_time:.2f} seconds.\")\n",
        "    return training_time\n",
        "\n",
        "def evaluate_model(model, testloader):\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        for inputs, labels in testloader:\n",
        "            inputs, labels = inputs.to(device), labels.to(device)\n",
        "            outputs = model(inputs)\n",
        "            _, predicted = torch.max(outputs, 1)\n",
        "            total += labels.size(0)\n",
        "            correct += (predicted == labels).sum().item()\n",
        "\n",
        "    accuracy = correct / total\n",
        "    print(f\"Validation Accuracy: {accuracy:.4f}\")\n",
        "    return accuracy\n",
        "\n",
        "epochs = 300\n",
        "print(\"Training the extended network on GPU\")\n",
        "training_time = train_model(model, trainloader, criterion, optimizer, epochs)\n",
        "\n",
        "print(\"Evaluating the extended network\")\n",
        "validation_accuracy = evaluate_model(model, testloader)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KeJ74Sjr7iCs",
        "outputId": "7be01511-2a31-424a-d913-690dd4977893"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using device: cuda\n",
            "Training the extended network on GPU\n",
            "Epoch 1/300, Loss: 1.6547\n",
            "Epoch 2/300, Loss: 1.4378\n",
            "Epoch 3/300, Loss: 1.3197\n",
            "Epoch 4/300, Loss: 1.2266\n",
            "Epoch 5/300, Loss: 1.1422\n",
            "Epoch 6/300, Loss: 1.0644\n",
            "Epoch 7/300, Loss: 0.9905\n",
            "Epoch 8/300, Loss: 0.9217\n",
            "Epoch 9/300, Loss: 0.8541\n",
            "Epoch 10/300, Loss: 0.7904\n",
            "Epoch 11/300, Loss: 0.7312\n",
            "Epoch 12/300, Loss: 0.6770\n",
            "Epoch 13/300, Loss: 0.6244\n",
            "Epoch 14/300, Loss: 0.5789\n",
            "Epoch 15/300, Loss: 0.5496\n",
            "Epoch 16/300, Loss: 0.5086\n",
            "Epoch 17/300, Loss: 0.4731\n",
            "Epoch 18/300, Loss: 0.4495\n",
            "Epoch 19/300, Loss: 0.4179\n",
            "Epoch 20/300, Loss: 0.3892\n",
            "Epoch 21/300, Loss: 0.3756\n",
            "Epoch 22/300, Loss: 0.3593\n",
            "Epoch 23/300, Loss: 0.3521\n",
            "Epoch 24/300, Loss: 0.3212\n",
            "Epoch 25/300, Loss: 0.3200\n",
            "Epoch 26/300, Loss: 0.3108\n",
            "Epoch 27/300, Loss: 0.2955\n",
            "Epoch 28/300, Loss: 0.2772\n",
            "Epoch 29/300, Loss: 0.2901\n",
            "Epoch 30/300, Loss: 0.2642\n",
            "Epoch 31/300, Loss: 0.2565\n",
            "Epoch 32/300, Loss: 0.2495\n",
            "Epoch 33/300, Loss: 0.2517\n",
            "Epoch 34/300, Loss: 0.2346\n",
            "Epoch 35/300, Loss: 0.2281\n",
            "Epoch 36/300, Loss: 0.2258\n",
            "Epoch 37/300, Loss: 0.2187\n",
            "Epoch 38/300, Loss: 0.2133\n",
            "Epoch 39/300, Loss: 0.2133\n",
            "Epoch 40/300, Loss: 0.2161\n",
            "Epoch 41/300, Loss: 0.2044\n",
            "Epoch 42/300, Loss: 0.2129\n",
            "Epoch 43/300, Loss: 0.1886\n",
            "Epoch 44/300, Loss: 0.2003\n",
            "Epoch 45/300, Loss: 0.1877\n",
            "Epoch 46/300, Loss: 0.1899\n",
            "Epoch 47/300, Loss: 0.1851\n",
            "Epoch 48/300, Loss: 0.1752\n",
            "Epoch 49/300, Loss: 0.1699\n",
            "Epoch 50/300, Loss: 0.1900\n",
            "Epoch 51/300, Loss: 0.1709\n",
            "Epoch 52/300, Loss: 0.1802\n",
            "Epoch 53/300, Loss: 0.1627\n",
            "Epoch 54/300, Loss: 0.1658\n",
            "Epoch 55/300, Loss: 0.1682\n",
            "Epoch 56/300, Loss: 0.1596\n",
            "Epoch 57/300, Loss: 0.1624\n",
            "Epoch 58/300, Loss: 0.1607\n",
            "Epoch 59/300, Loss: 0.1464\n",
            "Epoch 60/300, Loss: 0.1642\n",
            "Epoch 61/300, Loss: 0.1540\n",
            "Epoch 62/300, Loss: 0.1639\n",
            "Epoch 63/300, Loss: 0.1418\n",
            "Epoch 64/300, Loss: 0.1522\n",
            "Epoch 65/300, Loss: 0.1480\n",
            "Epoch 66/300, Loss: 0.1559\n",
            "Epoch 67/300, Loss: 0.1451\n",
            "Epoch 68/300, Loss: 0.1382\n",
            "Epoch 69/300, Loss: 0.1318\n",
            "Epoch 70/300, Loss: 0.1459\n",
            "Epoch 71/300, Loss: 0.1458\n",
            "Epoch 72/300, Loss: 0.1359\n",
            "Epoch 73/300, Loss: 0.1346\n",
            "Epoch 74/300, Loss: 0.1453\n",
            "Epoch 75/300, Loss: 0.1362\n",
            "Epoch 76/300, Loss: 0.1394\n",
            "Epoch 77/300, Loss: 0.1217\n",
            "Epoch 78/300, Loss: 0.1293\n",
            "Epoch 79/300, Loss: 0.1426\n",
            "Epoch 80/300, Loss: 0.1344\n",
            "Epoch 81/300, Loss: 0.1297\n",
            "Epoch 82/300, Loss: 0.1273\n",
            "Epoch 83/300, Loss: 0.1212\n",
            "Epoch 84/300, Loss: 0.1510\n",
            "Epoch 85/300, Loss: 0.1285\n",
            "Epoch 86/300, Loss: 0.1184\n",
            "Epoch 87/300, Loss: 0.1247\n",
            "Epoch 88/300, Loss: 0.1317\n",
            "Epoch 89/300, Loss: 0.1316\n",
            "Epoch 90/300, Loss: 0.1240\n",
            "Epoch 91/300, Loss: 0.1252\n",
            "Epoch 92/300, Loss: 0.1279\n",
            "Epoch 93/300, Loss: 0.1171\n",
            "Epoch 94/300, Loss: 0.1118\n",
            "Epoch 95/300, Loss: 0.1292\n",
            "Epoch 96/300, Loss: 0.1216\n",
            "Epoch 97/300, Loss: 0.1243\n",
            "Epoch 98/300, Loss: 0.1099\n",
            "Epoch 99/300, Loss: 0.1322\n",
            "Epoch 100/300, Loss: 0.1208\n",
            "Epoch 101/300, Loss: 0.1132\n",
            "Epoch 102/300, Loss: 0.1135\n",
            "Epoch 103/300, Loss: 0.1366\n",
            "Epoch 104/300, Loss: 0.1160\n",
            "Epoch 105/300, Loss: 0.1118\n",
            "Epoch 106/300, Loss: 0.1158\n",
            "Epoch 107/300, Loss: 0.1193\n",
            "Epoch 108/300, Loss: 0.1124\n",
            "Epoch 109/300, Loss: 0.1212\n",
            "Epoch 110/300, Loss: 0.1132\n",
            "Epoch 111/300, Loss: 0.1147\n",
            "Epoch 112/300, Loss: 0.1113\n",
            "Epoch 113/300, Loss: 0.1171\n",
            "Epoch 114/300, Loss: 0.1199\n",
            "Epoch 115/300, Loss: 0.1020\n",
            "Epoch 116/300, Loss: 0.1260\n",
            "Epoch 117/300, Loss: 0.1105\n",
            "Epoch 118/300, Loss: 0.1198\n",
            "Epoch 119/300, Loss: 0.1093\n",
            "Epoch 120/300, Loss: 0.1232\n",
            "Epoch 121/300, Loss: 0.1017\n",
            "Epoch 122/300, Loss: 0.1158\n",
            "Epoch 123/300, Loss: 0.1162\n",
            "Epoch 124/300, Loss: 0.1157\n",
            "Epoch 125/300, Loss: 0.0968\n",
            "Epoch 126/300, Loss: 0.1102\n",
            "Epoch 127/300, Loss: 0.1084\n",
            "Epoch 128/300, Loss: 0.1216\n",
            "Epoch 129/300, Loss: 0.1098\n",
            "Epoch 130/300, Loss: 0.1009\n",
            "Epoch 131/300, Loss: 0.0959\n",
            "Epoch 132/300, Loss: 0.1144\n",
            "Epoch 133/300, Loss: 0.1104\n",
            "Epoch 134/300, Loss: 0.1141\n",
            "Epoch 135/300, Loss: 0.1089\n",
            "Epoch 136/300, Loss: 0.1124\n",
            "Epoch 137/300, Loss: 0.1091\n",
            "Epoch 138/300, Loss: 0.0946\n",
            "Epoch 139/300, Loss: 0.1208\n",
            "Epoch 140/300, Loss: 0.1004\n",
            "Epoch 141/300, Loss: 0.1032\n",
            "Epoch 142/300, Loss: 0.1068\n",
            "Epoch 143/300, Loss: 0.1094\n",
            "Epoch 144/300, Loss: 0.0954\n",
            "Epoch 145/300, Loss: 0.1150\n",
            "Epoch 146/300, Loss: 0.1023\n",
            "Epoch 147/300, Loss: 0.1222\n",
            "Epoch 148/300, Loss: 0.0996\n",
            "Epoch 149/300, Loss: 0.0998\n",
            "Epoch 150/300, Loss: 0.1004\n",
            "Epoch 151/300, Loss: 0.1100\n",
            "Epoch 152/300, Loss: 0.1105\n",
            "Epoch 153/300, Loss: 0.1044\n",
            "Epoch 154/300, Loss: 0.1029\n",
            "Epoch 155/300, Loss: 0.1070\n",
            "Epoch 156/300, Loss: 0.0989\n",
            "Epoch 157/300, Loss: 0.0926\n",
            "Epoch 158/300, Loss: 0.1064\n",
            "Epoch 159/300, Loss: 0.1118\n",
            "Epoch 160/300, Loss: 0.1159\n",
            "Epoch 161/300, Loss: 0.0929\n",
            "Epoch 162/300, Loss: 0.0866\n",
            "Epoch 163/300, Loss: 0.1237\n",
            "Epoch 164/300, Loss: 0.0870\n",
            "Epoch 165/300, Loss: 0.1062\n",
            "Epoch 166/300, Loss: 0.1122\n",
            "Epoch 167/300, Loss: 0.0968\n",
            "Epoch 168/300, Loss: 0.1096\n",
            "Epoch 169/300, Loss: 0.0888\n",
            "Epoch 170/300, Loss: 0.1122\n",
            "Epoch 171/300, Loss: 0.0978\n",
            "Epoch 172/300, Loss: 0.1082\n",
            "Epoch 173/300, Loss: 0.1002\n",
            "Epoch 174/300, Loss: 0.1053\n",
            "Epoch 175/300, Loss: 0.1006\n",
            "Epoch 176/300, Loss: 0.1017\n",
            "Epoch 177/300, Loss: 0.0954\n",
            "Epoch 178/300, Loss: 0.0952\n",
            "Epoch 179/300, Loss: 0.1032\n",
            "Epoch 180/300, Loss: 0.1073\n",
            "Epoch 181/300, Loss: 0.0981\n",
            "Epoch 182/300, Loss: 0.0935\n",
            "Epoch 183/300, Loss: 0.1082\n",
            "Epoch 184/300, Loss: 0.1117\n",
            "Epoch 185/300, Loss: 0.0852\n",
            "Epoch 186/300, Loss: 0.1085\n",
            "Epoch 187/300, Loss: 0.0920\n",
            "Epoch 188/300, Loss: 0.1013\n",
            "Epoch 189/300, Loss: 0.1217\n",
            "Epoch 190/300, Loss: 0.0784\n",
            "Epoch 191/300, Loss: 0.1050\n",
            "Epoch 192/300, Loss: 0.1074\n",
            "Epoch 193/300, Loss: 0.0867\n",
            "Epoch 194/300, Loss: 0.1113\n",
            "Epoch 195/300, Loss: 0.0920\n",
            "Epoch 196/300, Loss: 0.1050\n",
            "Epoch 197/300, Loss: 0.0940\n",
            "Epoch 198/300, Loss: 0.0786\n",
            "Epoch 199/300, Loss: 0.0992\n",
            "Epoch 200/300, Loss: 0.0959\n",
            "Epoch 201/300, Loss: 0.1112\n",
            "Epoch 202/300, Loss: 0.0836\n",
            "Epoch 203/300, Loss: 0.0995\n",
            "Epoch 204/300, Loss: 0.1081\n",
            "Epoch 205/300, Loss: 0.0947\n",
            "Epoch 206/300, Loss: 0.0974\n",
            "Epoch 207/300, Loss: 0.0915\n",
            "Epoch 208/300, Loss: 0.0983\n",
            "Epoch 209/300, Loss: 0.1028\n",
            "Epoch 210/300, Loss: 0.0981\n",
            "Epoch 211/300, Loss: 0.0949\n",
            "Epoch 212/300, Loss: 0.0993\n",
            "Epoch 213/300, Loss: 0.1020\n",
            "Epoch 214/300, Loss: 0.1039\n",
            "Epoch 215/300, Loss: 0.0708\n",
            "Epoch 216/300, Loss: 0.1063\n",
            "Epoch 217/300, Loss: 0.0938\n",
            "Epoch 218/300, Loss: 0.0962\n",
            "Epoch 219/300, Loss: 0.1072\n",
            "Epoch 220/300, Loss: 0.0856\n",
            "Epoch 221/300, Loss: 0.1175\n",
            "Epoch 222/300, Loss: 0.0827\n",
            "Epoch 223/300, Loss: 0.1035\n",
            "Epoch 224/300, Loss: 0.0928\n",
            "Epoch 225/300, Loss: 0.1062\n",
            "Epoch 226/300, Loss: 0.0923\n",
            "Epoch 227/300, Loss: 0.0939\n",
            "Epoch 228/300, Loss: 0.1093\n",
            "Epoch 229/300, Loss: 0.0956\n",
            "Epoch 230/300, Loss: 0.1000\n",
            "Epoch 231/300, Loss: 0.0835\n",
            "Epoch 232/300, Loss: 0.0979\n",
            "Epoch 233/300, Loss: 0.0899\n",
            "Epoch 234/300, Loss: 0.1250\n",
            "Epoch 235/300, Loss: 0.0772\n",
            "Epoch 236/300, Loss: 0.1013\n",
            "Epoch 237/300, Loss: 0.0903\n",
            "Epoch 238/300, Loss: 0.0945\n",
            "Epoch 239/300, Loss: 0.0837\n",
            "Epoch 240/300, Loss: 0.0951\n",
            "Epoch 241/300, Loss: 0.1095\n",
            "Epoch 242/300, Loss: 0.1078\n",
            "Epoch 243/300, Loss: 0.0781\n",
            "Epoch 244/300, Loss: 0.0845\n",
            "Epoch 245/300, Loss: 0.0954\n",
            "Epoch 246/300, Loss: 0.1187\n",
            "Epoch 247/300, Loss: 0.0889\n",
            "Epoch 248/300, Loss: 0.0852\n",
            "Epoch 249/300, Loss: 0.1039\n",
            "Epoch 250/300, Loss: 0.1067\n",
            "Epoch 251/300, Loss: 0.0711\n",
            "Epoch 252/300, Loss: 0.1099\n",
            "Epoch 253/300, Loss: 0.0994\n",
            "Epoch 254/300, Loss: 0.0971\n",
            "Epoch 255/300, Loss: 0.0924\n",
            "Epoch 256/300, Loss: 0.0849\n",
            "Epoch 257/300, Loss: 0.1037\n",
            "Epoch 258/300, Loss: 0.1091\n",
            "Epoch 259/300, Loss: 0.0821\n",
            "Epoch 260/300, Loss: 0.0873\n",
            "Epoch 261/300, Loss: 0.1271\n",
            "Epoch 262/300, Loss: 0.0875\n",
            "Epoch 263/300, Loss: 0.0905\n",
            "Epoch 264/300, Loss: 0.0842\n",
            "Epoch 265/300, Loss: 0.0869\n",
            "Epoch 266/300, Loss: 0.1064\n",
            "Epoch 267/300, Loss: 0.0891\n",
            "Epoch 268/300, Loss: 0.0876\n",
            "Epoch 269/300, Loss: 0.0794\n",
            "Epoch 270/300, Loss: 0.1240\n",
            "Epoch 271/300, Loss: 0.0916\n",
            "Epoch 272/300, Loss: 0.0900\n",
            "Epoch 273/300, Loss: 0.0912\n",
            "Epoch 274/300, Loss: 0.1138\n",
            "Epoch 275/300, Loss: 0.0794\n",
            "Epoch 276/300, Loss: 0.0830\n",
            "Epoch 277/300, Loss: 0.1070\n",
            "Epoch 278/300, Loss: 0.0964\n",
            "Epoch 279/300, Loss: 0.1000\n",
            "Epoch 280/300, Loss: 0.0924\n",
            "Epoch 281/300, Loss: 0.0933\n",
            "Epoch 282/300, Loss: 0.1053\n",
            "Epoch 283/300, Loss: 0.0861\n",
            "Epoch 284/300, Loss: 0.0858\n",
            "Epoch 285/300, Loss: 0.0974\n",
            "Epoch 286/300, Loss: 0.1093\n",
            "Epoch 287/300, Loss: 0.0920\n",
            "Epoch 288/300, Loss: 0.0870\n",
            "Epoch 289/300, Loss: 0.0952\n",
            "Epoch 290/300, Loss: 0.0821\n",
            "Epoch 291/300, Loss: 0.1016\n",
            "Epoch 292/300, Loss: 0.0962\n",
            "Epoch 293/300, Loss: 0.1098\n",
            "Epoch 294/300, Loss: 0.0832\n",
            "Epoch 295/300, Loss: 0.1127\n",
            "Epoch 296/300, Loss: 0.0805\n",
            "Epoch 297/300, Loss: 0.0889\n",
            "Epoch 298/300, Loss: 0.0884\n",
            "Epoch 299/300, Loss: 0.1004\n",
            "Epoch 300/300, Loss: 0.1085\n",
            "Training completed in 3625.18 seconds.\n",
            "Evaluating the extended network\n",
            "Validation Accuracy: 0.5248\n"
          ]
        }
      ]
    }
  ]
}