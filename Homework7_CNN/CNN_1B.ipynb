{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "w7vKS5Pz2gUG",
        "outputId": "e93b22e1-d0bb-4f1a-f41e-f35c861dbc86"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 170M/170M [00:06<00:00, 26.0MB/s]\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "import time\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "transform_train = transforms.Compose([\n",
        "    transforms.RandomCrop(32, padding=4),\n",
        "    transforms.RandomHorizontalFlip(),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010))\n",
        "])\n",
        "\n",
        "transform_test = transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010))\n",
        "])\n",
        "\n",
        "trainset = torchvision.datasets.CIFAR10(root='./data', train=True,\n",
        "                                        download=True, transform=transform_train)\n",
        "trainloader = torch.utils.data.DataLoader(trainset, batch_size=128,\n",
        "                                          shuffle=True, num_workers=2)\n",
        "\n",
        "testset = torchvision.datasets.CIFAR10(root='./data', train=False,\n",
        "                                       download=True, transform=transform_test)\n",
        "testloader = torch.utils.data.DataLoader(testset, batch_size=100,\n",
        "                                         shuffle=False, num_workers=2)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Build the model architecture"
      ],
      "metadata": {
        "id": "brJrpIVSH8_B"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class DeeperCNN(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(DeeperCNN, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(3, 16, kernel_size=3, padding=1)\n",
        "        self.pool = nn.MaxPool2d(2,2)\n",
        "        self.conv2 = nn.Conv2d(16, 32, kernel_size=3, padding=1)\n",
        "        self.conv3 = nn.Conv2d(32, 64, kernel_size=3, padding=1)  # Adding an Extra Convolutional Layer\n",
        "\n",
        "        self.fc1 = nn.Linear(64 * 4 * 4, 128)\n",
        "        self.fc2 = nn.Linear(128, 10)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.pool(torch.relu(self.conv1(x))) # ReLu Activation Function\n",
        "        x = self.pool(torch.relu(self.conv2(x)))\n",
        "        x = self.pool(torch.relu(self.conv3(x)))\n",
        "        x = x.view(-1, 64 * 4 * 4)\n",
        "        x = torch.relu(self.fc1(x))\n",
        "        x = self.fc2(x)\n",
        "        return x\n",
        "\n",
        "model = DeeperCNN().to(device)\n",
        "\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.SGD(model.parameters(), lr=0.001, momentum=0.9) # Parameters"
      ],
      "metadata": {
        "id": "O-pweCVy2q56"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Train and Evaluate the model"
      ],
      "metadata": {
        "id": "ifRja6sd20KH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def train(model, loader, optimizer, criterion, device):\n",
        "    model.train()\n",
        "    running_loss = 0.0\n",
        "    for inputs, targets in loader:\n",
        "        inputs, targets = inputs.to(device), targets.to(device)\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(inputs)\n",
        "        loss = criterion(outputs, targets)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        running_loss += loss.item()*inputs.size(0)\n",
        "    epoch_loss = running_loss/len(loader.dataset)\n",
        "    return epoch_loss\n",
        "\n",
        "def test(model, loader, device):\n",
        "    model.eval()\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    with torch.no_grad():\n",
        "        for inputs, targets in loader:\n",
        "            inputs, targets = inputs.to(device), targets.to(device)\n",
        "            outputs = model(inputs)\n",
        "            _, predicted = torch.max(outputs, 1)\n",
        "            total += targets.size(0)\n",
        "            correct += predicted.eq(targets).sum().item()\n",
        "    acc = 100.*correct/total\n",
        "    return acc\n",
        "\n",
        "num_epochs = 200\n",
        "for epoch in range(num_epochs):\n",
        "    train_loss = train(model, trainloader, optimizer, criterion, device)\n",
        "    test_acc = test(model, testloader, device)\n",
        "    print(f\"Epoch [{epoch+1}/{num_epochs}] Loss: {train_loss:.4f}, Test Acc: {test_acc:.2f}%\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tZyj5Z6l20wf",
        "outputId": "830ae168-d6bd-44b7-b112-fa5074df7962"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [1/200] Loss: 2.2305, Test Acc: 25.31%\n",
            "Epoch [2/200] Loss: 2.0213, Test Acc: 33.07%\n",
            "Epoch [3/200] Loss: 1.8331, Test Acc: 37.27%\n",
            "Epoch [4/200] Loss: 1.6949, Test Acc: 42.22%\n",
            "Epoch [5/200] Loss: 1.6061, Test Acc: 44.93%\n",
            "Epoch [6/200] Loss: 1.5415, Test Acc: 47.12%\n",
            "Epoch [7/200] Loss: 1.4887, Test Acc: 49.44%\n",
            "Epoch [8/200] Loss: 1.4437, Test Acc: 51.79%\n",
            "Epoch [9/200] Loss: 1.4116, Test Acc: 53.42%\n",
            "Epoch [10/200] Loss: 1.3727, Test Acc: 53.73%\n",
            "Epoch [11/200] Loss: 1.3390, Test Acc: 55.35%\n",
            "Epoch [12/200] Loss: 1.2982, Test Acc: 56.69%\n",
            "Epoch [13/200] Loss: 1.2684, Test Acc: 58.31%\n",
            "Epoch [14/200] Loss: 1.2423, Test Acc: 59.66%\n",
            "Epoch [15/200] Loss: 1.2149, Test Acc: 59.52%\n",
            "Epoch [16/200] Loss: 1.1863, Test Acc: 61.21%\n",
            "Epoch [17/200] Loss: 1.1582, Test Acc: 61.45%\n",
            "Epoch [18/200] Loss: 1.1387, Test Acc: 63.39%\n",
            "Epoch [19/200] Loss: 1.1145, Test Acc: 64.30%\n",
            "Epoch [20/200] Loss: 1.0957, Test Acc: 64.48%\n",
            "Epoch [21/200] Loss: 1.0817, Test Acc: 62.83%\n",
            "Epoch [22/200] Loss: 1.0645, Test Acc: 64.12%\n",
            "Epoch [23/200] Loss: 1.0448, Test Acc: 66.43%\n",
            "Epoch [24/200] Loss: 1.0319, Test Acc: 66.57%\n",
            "Epoch [25/200] Loss: 1.0159, Test Acc: 67.56%\n",
            "Epoch [26/200] Loss: 1.0026, Test Acc: 68.03%\n",
            "Epoch [27/200] Loss: 0.9880, Test Acc: 67.99%\n",
            "Epoch [28/200] Loss: 0.9759, Test Acc: 67.72%\n",
            "Epoch [29/200] Loss: 0.9628, Test Acc: 69.03%\n",
            "Epoch [30/200] Loss: 0.9488, Test Acc: 68.51%\n",
            "Epoch [31/200] Loss: 0.9356, Test Acc: 69.84%\n",
            "Epoch [32/200] Loss: 0.9293, Test Acc: 70.21%\n",
            "Epoch [33/200] Loss: 0.9130, Test Acc: 70.34%\n",
            "Epoch [34/200] Loss: 0.9106, Test Acc: 70.56%\n",
            "Epoch [35/200] Loss: 0.8928, Test Acc: 70.47%\n",
            "Epoch [36/200] Loss: 0.8857, Test Acc: 71.10%\n",
            "Epoch [37/200] Loss: 0.8765, Test Acc: 71.48%\n",
            "Epoch [38/200] Loss: 0.8671, Test Acc: 72.10%\n",
            "Epoch [39/200] Loss: 0.8646, Test Acc: 72.21%\n",
            "Epoch [40/200] Loss: 0.8507, Test Acc: 71.95%\n",
            "Epoch [41/200] Loss: 0.8445, Test Acc: 72.57%\n",
            "Epoch [42/200] Loss: 0.8348, Test Acc: 72.40%\n",
            "Epoch [43/200] Loss: 0.8266, Test Acc: 72.55%\n",
            "Epoch [44/200] Loss: 0.8212, Test Acc: 73.42%\n",
            "Epoch [45/200] Loss: 0.8062, Test Acc: 73.72%\n",
            "Epoch [46/200] Loss: 0.8016, Test Acc: 73.94%\n",
            "Epoch [47/200] Loss: 0.8063, Test Acc: 73.83%\n",
            "Epoch [48/200] Loss: 0.7885, Test Acc: 73.46%\n",
            "Epoch [49/200] Loss: 0.7849, Test Acc: 74.75%\n",
            "Epoch [50/200] Loss: 0.7770, Test Acc: 73.83%\n",
            "Epoch [51/200] Loss: 0.7695, Test Acc: 75.06%\n",
            "Epoch [52/200] Loss: 0.7662, Test Acc: 74.71%\n",
            "Epoch [53/200] Loss: 0.7577, Test Acc: 75.06%\n",
            "Epoch [54/200] Loss: 0.7525, Test Acc: 75.10%\n",
            "Epoch [55/200] Loss: 0.7458, Test Acc: 75.77%\n",
            "Epoch [56/200] Loss: 0.7413, Test Acc: 75.23%\n",
            "Epoch [57/200] Loss: 0.7367, Test Acc: 75.43%\n",
            "Epoch [58/200] Loss: 0.7297, Test Acc: 75.08%\n",
            "Epoch [59/200] Loss: 0.7267, Test Acc: 75.27%\n",
            "Epoch [60/200] Loss: 0.7171, Test Acc: 75.82%\n",
            "Epoch [61/200] Loss: 0.7167, Test Acc: 75.51%\n",
            "Epoch [62/200] Loss: 0.7113, Test Acc: 75.73%\n",
            "Epoch [63/200] Loss: 0.7052, Test Acc: 76.28%\n",
            "Epoch [64/200] Loss: 0.7029, Test Acc: 76.35%\n",
            "Epoch [65/200] Loss: 0.6955, Test Acc: 76.72%\n",
            "Epoch [66/200] Loss: 0.6889, Test Acc: 76.14%\n",
            "Epoch [67/200] Loss: 0.6857, Test Acc: 77.04%\n",
            "Epoch [68/200] Loss: 0.6803, Test Acc: 76.54%\n",
            "Epoch [69/200] Loss: 0.6754, Test Acc: 76.58%\n",
            "Epoch [70/200] Loss: 0.6798, Test Acc: 76.74%\n",
            "Epoch [71/200] Loss: 0.6718, Test Acc: 76.58%\n",
            "Epoch [72/200] Loss: 0.6649, Test Acc: 76.85%\n",
            "Epoch [73/200] Loss: 0.6612, Test Acc: 76.49%\n",
            "Epoch [74/200] Loss: 0.6598, Test Acc: 76.33%\n",
            "Epoch [75/200] Loss: 0.6525, Test Acc: 77.36%\n",
            "Epoch [76/200] Loss: 0.6482, Test Acc: 76.56%\n",
            "Epoch [77/200] Loss: 0.6503, Test Acc: 77.43%\n",
            "Epoch [78/200] Loss: 0.6470, Test Acc: 77.32%\n",
            "Epoch [79/200] Loss: 0.6397, Test Acc: 77.06%\n",
            "Epoch [80/200] Loss: 0.6357, Test Acc: 76.67%\n",
            "Epoch [81/200] Loss: 0.6330, Test Acc: 76.99%\n",
            "Epoch [82/200] Loss: 0.6354, Test Acc: 77.25%\n",
            "Epoch [83/200] Loss: 0.6308, Test Acc: 77.26%\n",
            "Epoch [84/200] Loss: 0.6238, Test Acc: 77.84%\n",
            "Epoch [85/200] Loss: 0.6233, Test Acc: 77.61%\n",
            "Epoch [86/200] Loss: 0.6211, Test Acc: 77.80%\n",
            "Epoch [87/200] Loss: 0.6164, Test Acc: 77.48%\n",
            "Epoch [88/200] Loss: 0.6180, Test Acc: 77.88%\n",
            "Epoch [89/200] Loss: 0.6095, Test Acc: 77.95%\n",
            "Epoch [90/200] Loss: 0.6096, Test Acc: 77.50%\n",
            "Epoch [91/200] Loss: 0.6059, Test Acc: 77.73%\n",
            "Epoch [92/200] Loss: 0.6073, Test Acc: 78.37%\n",
            "Epoch [93/200] Loss: 0.5959, Test Acc: 78.14%\n",
            "Epoch [94/200] Loss: 0.5970, Test Acc: 78.49%\n",
            "Epoch [95/200] Loss: 0.5947, Test Acc: 78.17%\n",
            "Epoch [96/200] Loss: 0.5939, Test Acc: 78.42%\n",
            "Epoch [97/200] Loss: 0.5868, Test Acc: 77.84%\n",
            "Epoch [98/200] Loss: 0.5890, Test Acc: 78.88%\n",
            "Epoch [99/200] Loss: 0.5884, Test Acc: 78.03%\n",
            "Epoch [100/200] Loss: 0.5860, Test Acc: 78.56%\n",
            "Epoch [101/200] Loss: 0.5776, Test Acc: 78.60%\n",
            "Epoch [102/200] Loss: 0.5778, Test Acc: 77.97%\n",
            "Epoch [103/200] Loss: 0.5774, Test Acc: 78.67%\n",
            "Epoch [104/200] Loss: 0.5691, Test Acc: 78.47%\n",
            "Epoch [105/200] Loss: 0.5706, Test Acc: 78.83%\n",
            "Epoch [106/200] Loss: 0.5682, Test Acc: 77.71%\n",
            "Epoch [107/200] Loss: 0.5672, Test Acc: 79.32%\n",
            "Epoch [108/200] Loss: 0.5637, Test Acc: 78.71%\n",
            "Epoch [109/200] Loss: 0.5620, Test Acc: 78.98%\n",
            "Epoch [110/200] Loss: 0.5604, Test Acc: 79.53%\n",
            "Epoch [111/200] Loss: 0.5536, Test Acc: 79.04%\n",
            "Epoch [112/200] Loss: 0.5514, Test Acc: 79.38%\n",
            "Epoch [113/200] Loss: 0.5532, Test Acc: 79.21%\n",
            "Epoch [114/200] Loss: 0.5530, Test Acc: 79.12%\n",
            "Epoch [115/200] Loss: 0.5504, Test Acc: 79.55%\n",
            "Epoch [116/200] Loss: 0.5476, Test Acc: 79.66%\n",
            "Epoch [117/200] Loss: 0.5403, Test Acc: 79.50%\n",
            "Epoch [118/200] Loss: 0.5433, Test Acc: 79.57%\n",
            "Epoch [119/200] Loss: 0.5359, Test Acc: 79.94%\n",
            "Epoch [120/200] Loss: 0.5380, Test Acc: 79.45%\n",
            "Epoch [121/200] Loss: 0.5375, Test Acc: 79.33%\n",
            "Epoch [122/200] Loss: 0.5321, Test Acc: 79.59%\n",
            "Epoch [123/200] Loss: 0.5299, Test Acc: 79.62%\n",
            "Epoch [124/200] Loss: 0.5346, Test Acc: 79.20%\n",
            "Epoch [125/200] Loss: 0.5258, Test Acc: 79.78%\n",
            "Epoch [126/200] Loss: 0.5291, Test Acc: 79.56%\n",
            "Epoch [127/200] Loss: 0.5260, Test Acc: 79.39%\n",
            "Epoch [128/200] Loss: 0.5272, Test Acc: 79.59%\n",
            "Epoch [129/200] Loss: 0.5212, Test Acc: 79.72%\n",
            "Epoch [130/200] Loss: 0.5210, Test Acc: 79.62%\n",
            "Epoch [131/200] Loss: 0.5207, Test Acc: 79.64%\n",
            "Epoch [132/200] Loss: 0.5158, Test Acc: 80.13%\n",
            "Epoch [133/200] Loss: 0.5164, Test Acc: 79.66%\n",
            "Epoch [134/200] Loss: 0.5149, Test Acc: 79.62%\n",
            "Epoch [135/200] Loss: 0.5114, Test Acc: 79.56%\n",
            "Epoch [136/200] Loss: 0.5089, Test Acc: 80.00%\n",
            "Epoch [137/200] Loss: 0.5106, Test Acc: 78.85%\n",
            "Epoch [138/200] Loss: 0.5037, Test Acc: 79.75%\n",
            "Epoch [139/200] Loss: 0.5041, Test Acc: 79.77%\n",
            "Epoch [140/200] Loss: 0.5024, Test Acc: 80.00%\n",
            "Epoch [141/200] Loss: 0.5051, Test Acc: 80.42%\n",
            "Epoch [142/200] Loss: 0.5009, Test Acc: 80.40%\n",
            "Epoch [143/200] Loss: 0.5028, Test Acc: 80.10%\n",
            "Epoch [144/200] Loss: 0.4983, Test Acc: 79.96%\n",
            "Epoch [145/200] Loss: 0.4962, Test Acc: 80.00%\n",
            "Epoch [146/200] Loss: 0.4934, Test Acc: 79.59%\n",
            "Epoch [147/200] Loss: 0.4904, Test Acc: 79.95%\n",
            "Epoch [148/200] Loss: 0.4920, Test Acc: 80.16%\n",
            "Epoch [149/200] Loss: 0.4889, Test Acc: 80.58%\n",
            "Epoch [150/200] Loss: 0.4901, Test Acc: 80.48%\n",
            "Epoch [151/200] Loss: 0.4867, Test Acc: 80.17%\n",
            "Epoch [152/200] Loss: 0.4912, Test Acc: 80.51%\n",
            "Epoch [153/200] Loss: 0.4837, Test Acc: 80.63%\n",
            "Epoch [154/200] Loss: 0.4771, Test Acc: 80.12%\n",
            "Epoch [155/200] Loss: 0.4853, Test Acc: 80.09%\n",
            "Epoch [156/200] Loss: 0.4800, Test Acc: 80.32%\n",
            "Epoch [157/200] Loss: 0.4801, Test Acc: 80.91%\n",
            "Epoch [158/200] Loss: 0.4786, Test Acc: 79.45%\n",
            "Epoch [159/200] Loss: 0.4808, Test Acc: 80.18%\n",
            "Epoch [160/200] Loss: 0.4747, Test Acc: 80.40%\n",
            "Epoch [161/200] Loss: 0.4800, Test Acc: 80.22%\n",
            "Epoch [162/200] Loss: 0.4745, Test Acc: 80.76%\n",
            "Epoch [163/200] Loss: 0.4712, Test Acc: 80.50%\n",
            "Epoch [164/200] Loss: 0.4745, Test Acc: 80.47%\n",
            "Epoch [165/200] Loss: 0.4721, Test Acc: 80.24%\n",
            "Epoch [166/200] Loss: 0.4697, Test Acc: 80.68%\n",
            "Epoch [167/200] Loss: 0.4669, Test Acc: 80.27%\n",
            "Epoch [168/200] Loss: 0.4652, Test Acc: 80.56%\n",
            "Epoch [169/200] Loss: 0.4647, Test Acc: 80.39%\n",
            "Epoch [170/200] Loss: 0.4622, Test Acc: 80.04%\n",
            "Epoch [171/200] Loss: 0.4699, Test Acc: 80.42%\n",
            "Epoch [172/200] Loss: 0.4626, Test Acc: 80.65%\n",
            "Epoch [173/200] Loss: 0.4589, Test Acc: 80.81%\n",
            "Epoch [174/200] Loss: 0.4564, Test Acc: 80.56%\n",
            "Epoch [175/200] Loss: 0.4536, Test Acc: 80.74%\n",
            "Epoch [176/200] Loss: 0.4564, Test Acc: 80.54%\n",
            "Epoch [177/200] Loss: 0.4540, Test Acc: 81.00%\n",
            "Epoch [178/200] Loss: 0.4521, Test Acc: 80.36%\n",
            "Epoch [179/200] Loss: 0.4525, Test Acc: 81.03%\n",
            "Epoch [180/200] Loss: 0.4526, Test Acc: 81.00%\n",
            "Epoch [181/200] Loss: 0.4474, Test Acc: 80.32%\n",
            "Epoch [182/200] Loss: 0.4504, Test Acc: 80.27%\n",
            "Epoch [183/200] Loss: 0.4466, Test Acc: 81.01%\n",
            "Epoch [184/200] Loss: 0.4424, Test Acc: 80.40%\n",
            "Epoch [185/200] Loss: 0.4477, Test Acc: 80.54%\n",
            "Epoch [186/200] Loss: 0.4452, Test Acc: 80.42%\n",
            "Epoch [187/200] Loss: 0.4451, Test Acc: 80.45%\n",
            "Epoch [188/200] Loss: 0.4443, Test Acc: 81.22%\n",
            "Epoch [189/200] Loss: 0.4408, Test Acc: 81.17%\n",
            "Epoch [190/200] Loss: 0.4436, Test Acc: 80.70%\n",
            "Epoch [191/200] Loss: 0.4427, Test Acc: 80.96%\n",
            "Epoch [192/200] Loss: 0.4360, Test Acc: 80.67%\n",
            "Epoch [193/200] Loss: 0.4405, Test Acc: 80.70%\n",
            "Epoch [194/200] Loss: 0.4370, Test Acc: 81.08%\n",
            "Epoch [195/200] Loss: 0.4342, Test Acc: 80.74%\n",
            "Epoch [196/200] Loss: 0.4331, Test Acc: 81.31%\n",
            "Epoch [197/200] Loss: 0.4324, Test Acc: 80.97%\n",
            "Epoch [198/200] Loss: 0.4325, Test Acc: 81.40%\n",
            "Epoch [199/200] Loss: 0.4334, Test Acc: 80.82%\n",
            "Epoch [200/200] Loss: 0.4319, Test Acc: 81.20%\n"
          ]
        }
      ]
    }
  ]
}